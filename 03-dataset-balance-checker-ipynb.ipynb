{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67edd76c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-16T05:54:10.879812Z",
     "iopub.status.busy": "2025-07-16T05:54:10.878930Z",
     "iopub.status.idle": "2025-07-16T05:54:12.702365Z",
     "shell.execute_reply": "2025-07-16T05:54:12.701372Z"
    },
    "papermill": {
     "duration": 1.828635,
     "end_time": "2025-07-16T05:54:12.704194",
     "exception": false,
     "start_time": "2025-07-16T05:54:10.875559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0f97e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T05:54:12.709590Z",
     "iopub.status.busy": "2025-07-16T05:54:12.708750Z",
     "iopub.status.idle": "2025-07-16T05:54:12.719278Z",
     "shell.execute_reply": "2025-07-16T05:54:12.718204Z"
    },
    "papermill": {
     "duration": 0.014447,
     "end_time": "2025-07-16T05:54:12.720582",
     "exception": true,
     "start_time": "2025-07-16T05:54:12.706135",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character 'ðŸ“Œ' (U+1F4CC) (1211225527.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_13/1211225527.py\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    ðŸ“Œ 1. Load Libraries\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character 'ðŸ“Œ' (U+1F4CC)\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Š AGI Voice Agent - Dataset Balance Checker (Notebook 3)\n",
    "\n",
    "### Purpose:\n",
    "Analyze all 73 datasets to:\n",
    "- Check size (rows, columns)\n",
    "- Identify class/label balance (for classification datasets)\n",
    "- Detect null/missing data\n",
    "- Save a report for monitoring dataset health\n",
    "\n",
    "\n",
    "\n",
    "ðŸ“Œ 1. Load Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ðŸ“Œ 2. Define Dataset Paths\n",
    "\n",
    "dataset_files = {\n",
    "    \"AI_Reasoning\": \"processed/ai_reasoning.parquet\",\n",
    "    \"Hellaswag_Test\": \"processed/hellaswag_test_cleaned.jsonl\",\n",
    "    \"Hellaswag_Val\": \"processed/hellaswag_val_cleaned.jsonl\",\n",
    "    # Extend up to 73 datasets\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ðŸ“Œ 3. Helper Functions\n",
    "\n",
    "def analyze_csv_or_parquet(file_path):\n",
    "    df = pd.read_parquet(file_path) if file_path.endswith('.parquet') else pd.read_csv(file_path)\n",
    "    return {\n",
    "        'rows': len(df),\n",
    "        'columns': df.columns.tolist(),\n",
    "        'nulls': df.isnull().sum().to_dict(),\n",
    "        'sample': df.head().to_dict()\n",
    "    }\n",
    "\n",
    "def analyze_jsonl(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return {\n",
    "        'records': len(data),\n",
    "        'sample': data[:2]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ðŸ“Œ 4. Run Balance Checker on All Datasets\n",
    "\n",
    "dataset_analysis = {}\n",
    "\n",
    "for name, path in dataset_files.items():\n",
    "    print(f\"Analyzing {name}...\")\n",
    "    if path.endswith('.parquet') or path.endswith('.csv'):\n",
    "        analysis = analyze_csv_or_parquet(path)\n",
    "    elif path.endswith('.jsonl'):\n",
    "        analysis = analyze_jsonl(path)\n",
    "    else:\n",
    "        analysis = {\"error\": \"Unsupported file format\"}\n",
    "    \n",
    "    dataset_analysis[name] = analysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ðŸ“Œ 5. Display a Summary\n",
    "\n",
    "for name, analysis in dataset_analysis.items():\n",
    "    print(f\"Dataset: {name}\")\n",
    "    if 'rows' in analysis:\n",
    "        print(f\"- Rows: {analysis['rows']}\")\n",
    "        print(f\"- Columns: {analysis['columns']}\")\n",
    "        print(f\"- Nulls: {analysis['nulls']}\")\n",
    "    elif 'records' in analysis:\n",
    "        print(f\"- Records: {analysis['records']}\")\n",
    "    print(f\"- Sample: {analysis['sample']}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ðŸ“Œ 6. Save the Analysis Report\n",
    "\n",
    "import json\n",
    "\n",
    "Path(\"reports\").mkdir(exist_ok=True)\n",
    "\n",
    "with open('reports/dataset_balance_report.json', 'w') as f:\n",
    "    json.dump(dataset_analysis, f, indent=4)\n",
    "\n",
    "print(\"ðŸ“Š Dataset balance report saved at reports/dataset_balance_report.json\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.22356,
   "end_time": "2025-07-16T05:54:13.241557",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-16T05:54:06.017997",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
