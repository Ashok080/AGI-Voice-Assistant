{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b53d78",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-16T07:44:13.131714Z",
     "iopub.status.busy": "2025-07-16T07:44:13.131311Z",
     "iopub.status.idle": "2025-07-16T07:44:15.156596Z",
     "shell.execute_reply": "2025-07-16T07:44:15.155404Z"
    },
    "papermill": {
     "duration": 2.03099,
     "end_time": "2025-07-16T07:44:15.159052",
     "exception": false,
     "start_time": "2025-07-16T07:44:13.128062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5149c102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T07:44:15.164516Z",
     "iopub.status.busy": "2025-07-16T07:44:15.163885Z",
     "iopub.status.idle": "2025-07-16T07:44:15.175750Z",
     "shell.execute_reply": "2025-07-16T07:44:15.174577Z"
    },
    "papermill": {
     "duration": 0.016474,
     "end_time": "2025-07-16T07:44:15.177389",
     "exception": true,
     "start_time": "2025-07-16T07:44:15.160915",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '✅' (U+2705) (1684979588.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_13/1684979588.py\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    ✅ 1. Detect and Configure Device\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '✅' (U+2705)\n"
     ]
    }
   ],
   "source": [
    "# ⚡ AGI Voice Agent - GPU & CPU Optimization (Notebook 23)\n",
    "\n",
    "### Objective:\n",
    "- Maximize performance on both CPU & GPU\n",
    "- Enable device selection (CUDA or CPU)\n",
    "- Reduce latency for inference\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "✅ 1. Detect and Configure Device\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "✅ 2. Load Models with Device Assignment\n",
    "\n",
    "Whisper (for Voice-to-Text):\n",
    "\n",
    "import whisper\n",
    "\n",
    "whisper_model = whisper.load_model(\"base\").to(device)\n",
    "\n",
    "Transformers (Reasoning Model):\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "✅ 3. Efficient Inference with FP16 (GPU)\n",
    "\n",
    "model.half()  # Reduces memory and speeds up on GPU\n",
    "\n",
    "input_ids = tokenizer(\"Define artificial general intelligence.\", return_tensors='pt').input_ids.to(device)\n",
    "outputs = model.generate(input_ids, max_length=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)\n",
    "\n",
    "\n",
    "\n",
    "✅ 4. Optimize CPU with ONNX Runtime\n",
    "\n",
    "pip install onnx onnxruntime\n",
    "\n",
    "Convert transformer to ONNX:\n",
    "\n",
    "from transformers.onnx import export\n",
    "from pathlib import Path\n",
    "\n",
    "export(\n",
    "    preprocessor=tokenizer,\n",
    "    model=model,\n",
    "    config=model.config,\n",
    "    opset=12,\n",
    "    output=Path(\"gpt2.onnx\"),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "Run with ONNX Runtime:\n",
    "\n",
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession(\"gpt2.onnx\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "✅ 5. Benchmarking CPU vs GPU\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "_ = model.generate(input_ids, max_length=100)\n",
    "end = time.time()\n",
    "print(f\"Inference Time: {end - start:.2f} seconds on {device}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "✅ 6. Bonus: Parallel GPU Inference with DeepSpeed / Huggingface Accelerate\n",
    "\n",
    "pip install deepspeed accelerate\n",
    "\n",
    "Initialize with DeepSpeed configs for multi-GPU inference."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.646327,
   "end_time": "2025-07-16T07:44:15.699321",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-16T07:44:08.052994",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
